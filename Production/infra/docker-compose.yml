# =============================================================================
# docker-compose.yml — Production Kafka CDC Migration Stack
# =============================================================================
# All credentials and ports are sourced from .env (copy from .env.example).
# Run: docker compose --env-file .env up -d
#
# Services:
#   kafka            — Broker 1 + KRaft controller (combined mode)
#   kafka-broker-2   — Broker 2 (broker-only)
#   schema-registry  — Karapace (open-source Schema Registry)
#   kafka-connect    — Debezium Connect with Confluent Avro converter
#   sqlserver        — SQL Server 2019 (CDC source)
#   postgres         — PostgreSQL 15 (CDC source, logical replication)
#   kafka-ui         — Observability UI
# =============================================================================

services:

  # ---------------------------------------------------------------------------
  # Kafka Broker 1 — KRaft combined mode (broker + controller)
  #
  # Decision: KRaft over ZooKeeper
  #   ZooKeeper is removed from Kafka 3.x+ and is a separate failure domain.
  #   KRaft (Kafka Raft) combines metadata management into the broker process,
  #   reducing operational complexity and eliminating the ZooKeeper dependency.
  #   Confluent 7.7.x fully supports KRaft for production workloads.
  #
  # Decision: pinned image tag (7.7.7)
  #   Using :latest in production causes silent breaking changes on restart.
  #   The pinned tag guarantees reproducible deployments.
  # ---------------------------------------------------------------------------
  kafka:
    image: confluentinc/cp-kafka:7.7.7
    container_name: kafka
    restart: unless-stopped
    ports:
      - "${KAFKA_EXTERNAL_PORT:-9092}:9092"
    environment:
      KAFKA_NODE_ID: 1
      KAFKA_PROCESS_ROLES: broker,controller

      # Listener protocol map — PLAINTEXT only (add SSL listener for prod TLS)
      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: CONTROLLER:PLAINTEXT,PLAINTEXT:PLAINTEXT,PLAINTEXT_HOST:PLAINTEXT

      # Internal address (Docker network) + external address (host)
      KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://kafka:29092,PLAINTEXT_HOST://localhost:${KAFKA_EXTERNAL_PORT:-9092}
      KAFKA_LISTENERS: PLAINTEXT://kafka:29092,CONTROLLER://kafka:29093,PLAINTEXT_HOST://0.0.0.0:9092

      KAFKA_INTER_BROKER_LISTENER_NAME: PLAINTEXT
      KAFKA_CONTROLLER_LISTENER_NAMES: CONTROLLER
      KAFKA_CONTROLLER_QUORUM_VOTERS: 1@kafka:29093

      # Decision: replication factor = 2 (matches 2-broker cluster)
      #   RF=1 (the dev default) means a single broker failure loses all data.
      #   RF=2 with 2 brokers survives one broker restart. Increase to 3 if
      #   adding a third broker.
      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 2
      KAFKA_DEFAULT_REPLICATION_FACTOR: 2
      KAFKA_TRANSACTION_STATE_LOG_REPLICATION_FACTOR: 2
      KAFKA_TRANSACTION_STATE_LOG_MIN_ISR: 2

      KAFKA_GROUP_INITIAL_REBALANCE_DELAY_MS: 0

      # Decision: log retention = 7 days / 50 GB per partition
      #   Migration data should be retained long enough for downstream consumers
      #   to catch up after snapshot. 7 days is a safe default. Tune per table size.
      KAFKA_LOG_RETENTION_HOURS: 168
      KAFKA_LOG_RETENTION_BYTES: 53687091200
      KAFKA_LOG_SEGMENT_BYTES: 536870912

      # Decision: compression = lz4 at broker level
      #   lz4 provides ~20-30% space savings on Avro-encoded CDC records with
      #   negligible CPU overhead (<2% on modern hardware). Reduces broker disk
      #   I/O and network bandwidth between brokers during replication.
      KAFKA_COMPRESSION_TYPE: lz4

      # Decision: JVM heap via env var
      #   Default Kafka heap (1 GB) causes GC pauses at sustained 40 MB/s.
      #   6 GB covers page cache pressure from the migration workload.
      KAFKA_HEAP_OPTS: "${KAFKA_HEAP_OPTS:--Xmx4g -Xms4g}"

      KAFKA_LOG_DIRS: /var/lib/kafka/data
      CLUSTER_ID: "${KAFKA_CLUSTER_ID}"
    volumes:
      - kafka-data:/var/lib/kafka/data
    healthcheck:
      test: kafka-broker-api-versions --bootstrap-server localhost:9092 > /dev/null 2>&1 && echo OK
      interval: 15s
      timeout: 10s
      retries: 10
      start_period: 30s

  # ---------------------------------------------------------------------------
  # Kafka Broker 2 — broker-only node
  #
  # Decision: 2 brokers minimum
  #   A single-broker cluster loses all data on restart (RF=1 forced).
  #   Two brokers allow RF=2 so each partition has a replica on each broker.
  #   Both brokers must have the same CLUSTER_ID.
  # ---------------------------------------------------------------------------
  kafka-broker-2:
    image: confluentinc/cp-kafka:7.7.7
    container_name: kafka-broker-2
    restart: unless-stopped
    ports:
      - "${KAFKA_BROKER2_EXTERNAL_PORT:-9093}:9093"
    depends_on:
      kafka:
        condition: service_healthy
    environment:
      KAFKA_NODE_ID: 2
      KAFKA_PROCESS_ROLES: broker
      KAFKA_CONTROLLER_QUORUM_VOTERS: 1@kafka:29093
      KAFKA_CONTROLLER_LISTENER_NAMES: CONTROLLER
      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: CONTROLLER:PLAINTEXT,PLAINTEXT:PLAINTEXT,PLAINTEXT_HOST:PLAINTEXT
      KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://kafka-broker-2:29092,PLAINTEXT_HOST://localhost:${KAFKA_BROKER2_EXTERNAL_PORT:-9093}
      KAFKA_LISTENERS: PLAINTEXT://kafka-broker-2:29092,PLAINTEXT_HOST://0.0.0.0:9093
      KAFKA_INTER_BROKER_LISTENER_NAME: PLAINTEXT
      KAFKA_LOG_DIRS: /var/lib/kafka/data
      KAFKA_HEAP_OPTS: "${KAFKA_HEAP_OPTS:--Xmx4g -Xms4g}"
      KAFKA_COMPRESSION_TYPE: lz4
      CLUSTER_ID: "${KAFKA_CLUSTER_ID}"
    volumes:
      - kafka-broker-2-data:/var/lib/kafka/data
    healthcheck:
      test: kafka-broker-api-versions --bootstrap-server localhost:9093 > /dev/null 2>&1 && echo OK
      interval: 15s
      timeout: 10s
      retries: 10
      start_period: 30s

  # ---------------------------------------------------------------------------
  # Karapace Schema Registry
  #
  # Decision: Karapace over Confluent Schema Registry
  #   Confluent Schema Registry requires a commercial license for production.
  #   Karapace is wire-compatible (same REST API, same Avro/JSON/Protobuf support)
  #   and is Apache 2.0 licensed. The Debezium AvroConverter and all Confluent
  #   client libs work against it transparently.
  #
  # Decision: both brokers in bootstrap URI
  #   Schema Registry stores schemas in an internal Kafka topic. If only one
  #   broker is listed and that broker restarts, schema registration fails mid-
  #   migration. Both brokers in the URI provides automatic failover.
  # ---------------------------------------------------------------------------
  schema-registry:
    image: ghcr.io/aiven-open/karapace:latest
    container_name: schema-registry
    restart: unless-stopped
    command: ["python3", "-m", "karapace"]
    depends_on:
      kafka:
        condition: service_healthy
      kafka-broker-2:
        condition: service_healthy
    ports:
      - "${SCHEMA_REGISTRY_EXTERNAL_PORT:-8081}:8081"
    environment:
      KARAPACE_ADVERTISED_HOSTNAME: schema-registry
      KARAPACE_BOOTSTRAP_URI: kafka:29092,kafka-broker-2:29092
      KARAPACE_HOST: 0.0.0.0
      KARAPACE_PORT: 8081
      # Decision: BACKWARD compatibility (default)
      #   Prevents schema changes that break existing consumers.
      #   Allows adding optional fields (nullable with default). Never set NONE.
      KARAPACE_COMPATIBILITY: BACKWARD
    healthcheck:
      test: curl -sf http://localhost:8081/subjects > /dev/null && echo OK
      interval: 15s
      timeout: 5s
      retries: 10
      start_period: 20s

  # ---------------------------------------------------------------------------
  # Debezium Kafka Connect
  #
  # Decision: custom Dockerfile (Debezium base + Confluent Avro JARs)
  #   Debezium/connect:2.3 ships without the Confluent AvroConverter. Rather
  #   than downloading JARs at runtime (network dependency, non-reproducible),
  #   we COPY the full confluent-avro plugin directory from the official
  #   Confluent Kafka Connect image at build time. The image is immutable and
  #   self-contained.
  #
  # Decision: JVM heap 2 GB minimum
  #   The benchmark showed GC pause stalls with the default 256 MB heap when
  #   batch=2048 and 4 concurrent tasks. 2 GB gives the GC enough headroom for
  #   smooth, low-pause operation under migration load.
  #
  # Decision: RF=2 for all Connect internal topics
  #   connect-offsets stores the CDC offset (LSN / WAL position). If it is lost,
  #   the connector restarts from snapshot — re-reading the entire table.
  #   RF=2 prevents this on a single broker failure.
  # ---------------------------------------------------------------------------
  kafka-connect:
    build:
      context: ./connect
      dockerfile: Dockerfile
    image: debezium-connect-avro:2.3
    container_name: kafka-connect
    restart: unless-stopped
    depends_on:
      kafka:
        condition: service_healthy
      kafka-broker-2:
        condition: service_healthy
      schema-registry:
        condition: service_healthy
    ports:
      - "${CONNECT_PORT:-8083}:8083"
    environment:
      GROUP_ID: "kafka-connect-prod"
      BOOTSTRAP_SERVERS: kafka:29092,kafka-broker-2:29092

      CONFIG_STORAGE_TOPIC: connect-configs
      OFFSET_STORAGE_TOPIC: connect-offsets
      STATUS_STORAGE_TOPIC: connect-status

      # Decision: RF=2 for Connect internal topics (see above)
      CONFIG_STORAGE_REPLICATION_FACTOR: "2"
      OFFSET_STORAGE_REPLICATION_FACTOR: "2"
      STATUS_STORAGE_REPLICATION_FACTOR: "2"

      # Decision: Avro converter globally on the worker
      #   Setting converters at the worker level means all connectors inherit
      #   Avro by default. Individual connectors can still override per-connector,
      #   but the safe default is Avro everywhere — not JSON, which embeds full
      #   schemas in every message (~4× larger).
      KEY_CONVERTER: io.confluent.connect.avro.AvroConverter
      VALUE_CONVERTER: io.confluent.connect.avro.AvroConverter
      CONNECT_KEY_CONVERTER_SCHEMA_REGISTRY_URL: http://schema-registry:8081
      CONNECT_VALUE_CONVERTER_SCHEMA_REGISTRY_URL: http://schema-registry:8081

      # Decision: Connect JVM heap
      KAFKA_HEAP_OPTS: "${CONNECT_HEAP_OPTS:--Xmx2g -Xms2g}"

      # Decision: offset flush every 10s (not the 60s default)
      #   If the Connect worker crashes, it will re-process records since the
      #   last committed offset. Flushing every 10s (vs 60s default) reduces the
      #   reprocessing window from up to 60s to at most 10s of duplicate events.
      OFFSET_FLUSH_INTERVAL_MS: "10000"
    healthcheck:
      test: curl -sf http://localhost:8083/connectors > /dev/null && echo OK
      interval: 15s
      timeout: 10s
      retries: 15
      start_period: 60s

  # ---------------------------------------------------------------------------
  # SQL Server 2019 (Developer Edition)
  #
  # Decision: SQL Server Agent enabled
  #   CDC in SQL Server depends on the SQL Server Agent to run the capture job
  #   that reads from the transaction log and writes to change tables. Without
  #   Agent, CDC tables are never populated and Debezium reads nothing.
  #
  # Decision: Snapshot isolation enabled at OS level via sqlserver/setup.sql
  #   snapshot.isolation.level=snapshot in the connector requires that the
  #   database has READ_COMMITTED_SNAPSHOT and ALLOW_SNAPSHOT_ISOLATION enabled.
  #   Without this, Debezium takes row-level shared locks during snapshot,
  #   blocking all writes on the source table for the duration (hours on large
  #   tables). Snapshot isolation uses TempDB row versioning instead — no locks.
  #
  # Decision: named volume for data persistence
  #   /var/opt/mssql contains the MDF/LDF files. Without a named volume, a
  #   container restart loses the entire database including CDC change tables
  #   and the Debezium offset checkpoint.
  # ---------------------------------------------------------------------------
  sqlserver:
    image: mcr.microsoft.com/mssql/server:2019-latest
    container_name: sqlserver
    restart: unless-stopped
    ports:
      - "${MSSQL_PORT:-1433}:1433"
    environment:
      ACCEPT_EULA: "Y"
      MSSQL_SA_PASSWORD: "${MSSQL_SA_PASSWORD}"
      MSSQL_AGENT_ENABLED: "true"
      MSSQL_PID: Developer
    volumes:
      - sqlserver-data:/var/opt/mssql
      - ./sqlserver:/scripts
    healthcheck:
      test: /opt/mssql-tools18/bin/sqlcmd -S localhost -U sa -P "${MSSQL_SA_PASSWORD}" -Q "SELECT 1" -b -o /dev/null -C
      interval: 10s
      timeout: 5s
      retries: 12
      start_period: 40s

  # ---------------------------------------------------------------------------
  # PostgreSQL 15
  #
  # Decision: wal_level=logical (required for Debezium pgoutput)
  #   The default wal_level=replica only writes row-level changes for physical
  #   replication. logical adds column-level before/after images needed by
  #   logical replication slots that Debezium uses to read changes.
  #
  # Decision: max_replication_slots=10
  #   Each Debezium PostgreSQL connector consumes one replication slot. Running
  #   10 connectors in parallel (one per table) requires 10 slots. Unused slots
  #   must be dropped — they prevent WAL segment recycling (disk fill risk).
  #
  # Decision: max_wal_senders=10
  #   One WAL sender process per active replication slot. Must be >= slots.
  # ---------------------------------------------------------------------------
  postgres:
    image: postgres:15
    container_name: postgres
    restart: unless-stopped
    ports:
      - "${POSTGRES_PORT:-5432}:5432"
    environment:
      POSTGRES_USER: "${POSTGRES_USER}"
      POSTGRES_PASSWORD: "${POSTGRES_PASSWORD}"
      POSTGRES_DB: "${POSTGRES_DB:-benchdb}"
    command: >
      postgres
        -c wal_level=logical
        -c max_replication_slots=10
        -c max_wal_senders=10
        -c shared_buffers=512MB
        -c work_mem=16MB
        -c maintenance_work_mem=256MB
        -c checkpoint_completion_target=0.9
        -c wal_buffers=64MB
        -c synchronous_commit=off
    volumes:
      - postgres-data:/var/lib/postgresql/data
      - ./postgres:/docker-entrypoint-initdb.d
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U ${POSTGRES_USER} -d ${POSTGRES_DB:-benchdb}"]
      interval: 10s
      timeout: 5s
      retries: 10
      start_period: 20s

  # ---------------------------------------------------------------------------
  # Kafka UI (Provectus)
  # Read-only observability — not a data path component.
  # ---------------------------------------------------------------------------
  kafka-ui:
    image: provectuslabs/kafka-ui:latest
    container_name: kafka-ui
    restart: unless-stopped
    depends_on:
      kafka:
        condition: service_healthy
      schema-registry:
        condition: service_healthy
      kafka-connect:
        condition: service_healthy
    ports:
      - "${KAFKA_UI_PORT:-8084}:8080"
    environment:
      KAFKA_CLUSTERS_0_NAME: production
      KAFKA_CLUSTERS_0_BOOTSTRAPSERVERS: kafka:29092,kafka-broker-2:29092
      KAFKA_CLUSTERS_0_SCHEMAREGISTRY: http://schema-registry:8081
      KAFKA_CLUSTERS_0_KAFKACONNECT_0_NAME: debezium
      KAFKA_CLUSTERS_0_KAFKACONNECT_0_ADDRESS: http://kafka-connect:8083

# =============================================================================
# Named volumes — all data persisted across container restarts
# =============================================================================
volumes:
  kafka-data:
  kafka-broker-2-data:
  sqlserver-data:
  postgres-data:
